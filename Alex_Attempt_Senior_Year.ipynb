{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Images\n",
    "from matplotlib.pyplot import imread\n",
    "from scipy.misc import toimage\n",
    "import cv2 as cv\n",
    "from skimage.feature import canny\n",
    "from skimage.transform import hough_line, hough_line_peaks, rotate\n",
    "from PIL import Image\n",
    "from skimage import filters\n",
    "\n",
    "#### Graph\n",
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.path as mplPath\n",
    "\n",
    "#### Strings\n",
    "from StringIO import StringIO\n",
    "from fuzzywuzzy import fuzz, process\n",
    "import re\n",
    "from textblob import TextBlob, Word\n",
    "import difflib\n",
    "import editdistance\n",
    "import re\n",
    "\n",
    "#### Data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pytesseract\n",
    "\n",
    "#### Misc\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "from pprint import pprint\n",
    "import os\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "from collections import Counter\n",
    "\n",
    "tqdm_notebook.pandas()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiprocessing Apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _apply_df(args):\n",
    "    df, func, kwargs = args\n",
    "    return df.progress_apply(func, **kwargs)\n",
    "\n",
    "def apply_by_multiprocessing(df, func, **kwargs):\n",
    "    workers = kwargs.pop('workers')\n",
    "    pool = multiprocessing.Pool(processes=workers)\n",
    "    result = pool.map(_apply_df, [(d, func, kwargs)\n",
    "            for d in np.array_split(df, workers)])\n",
    "    pool.close()\n",
    "    return pd.concat(list(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "root = '/home/localhost/Desktop/Projects/Working/PeabodyNotecards/peabody_files/Accession_Files/'\n",
    "accessions = filter(lambda x: x != \".DS_Store\", os.listdir(root))\n",
    "paths = []\n",
    "errors = {}\n",
    "for acc in tqdm(accessions):\n",
    "    acc_path = root+acc\n",
    "    acc_images = filter(lambda x: x != \".DS_Store\", os.listdir(acc_path))\n",
    "    for image in tqdm(acc_images, leave=False):\n",
    "        img_path = acc_path+'/'+image\n",
    "        paths.append(img_path)\n",
    "print('Loaded', len(paths), \"image paths\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['SiteNo'] == 'M5942']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = 'peabody_files/Accession_Files/16/25_2654.pdf.png'\n",
    "def load_image(path):\n",
    "    img = imread(path)\n",
    "    if len(img.shape) != 2:\n",
    "        img = cv.cvtColor(img, cv.COLOR_BGRA2GRAY) # Convert color space\n",
    "    if np.max(img) <= 1.1:\n",
    "        img *= 255\n",
    "    img = img.astype('uint8')\n",
    "    return img\n",
    "image = load_image(test_image)\n",
    "imshow(image, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize(img):\n",
    "    return cv.threshold(cv.GaussianBlur(img,(5,5),0),0,255,cv.THRESH_BINARY+cv.THRESH_OTSU)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = binarize(image)\n",
    "imshow(image, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise Removal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_noise(img):\n",
    "    return cv.fastNlMeansDenoising(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = remove_noise(image)\n",
    "imshow(image, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rotation / Deskewing\n",
    "##### Creds to https://github.com/kakul/Alyn/blob/master/alyn/skew_detect.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1,
     16,
     26,
     38
    ]
   },
   "outputs": [],
   "source": [
    "def deskew(img):\n",
    "    #### Estimate Angle\n",
    "    piby4 = np.pi / 4\n",
    "    def get_max_freq_elem(arr):\n",
    "        max_arr = []\n",
    "        freqs = {}\n",
    "        for i in arr:\n",
    "            if i in freqs:\n",
    "                freqs[i] += 1\n",
    "            else:\n",
    "                freqs[i] = 1\n",
    "        sorted_keys = sorted(freqs, key=freqs.get, reverse=True)\n",
    "        max_freq = freqs[sorted_keys[0]]\n",
    "        for k in sorted_keys:\n",
    "            if freqs[k] == max_freq:\n",
    "                max_arr.append(k)\n",
    "        return max_arr\n",
    "\n",
    "    def calculate_deviation(angle):\n",
    "        angle_in_degrees = np.abs(angle)\n",
    "        deviation = np.abs(piby4 - angle_in_degrees)\n",
    "        return deviation\n",
    "    compare_sum = lambda x: x>=44 and x <=46\n",
    "\n",
    "    edges = canny(img, sigma=3)\n",
    "    h, a, d = hough_line(edges)\n",
    "    _, ap, _ = hough_line_peaks(h, a, d, num_peaks=20)\n",
    "\n",
    "    if len(ap) == 0:\n",
    "        raise ValueError(\"BAD IMAGE\")\n",
    "\n",
    "    absolute_deviations = [calculate_deviation(k) for k in ap]\n",
    "    average_deviation = np.mean(np.rad2deg(absolute_deviations))\n",
    "    ap_deg = [np.rad2deg(x) for x in ap]\n",
    "\n",
    "    bin_0_45 = []\n",
    "    bin_45_90 = []\n",
    "    bin_0_45n = []\n",
    "    bin_45_90n = []\n",
    "\n",
    "    for ang in ap_deg:\n",
    "        deviation_sum = int(90 - ang + average_deviation)\n",
    "        if compare_sum(deviation_sum):\n",
    "            bin_45_90.append(ang)\n",
    "            continue\n",
    "        deviation_sum = int(ang + average_deviation)\n",
    "        if compare_sum(deviation_sum):\n",
    "            bin_0_45.append(ang)\n",
    "            continue\n",
    "        deviation_sum = int(-ang + average_deviation)\n",
    "        if compare_sum(deviation_sum):\n",
    "            bin_0_45n.append(ang)\n",
    "            continue\n",
    "        deviation_sum = int(90 + ang + average_deviation)\n",
    "        if compare_sum(deviation_sum):\n",
    "            bin_45_90n.append(ang)\n",
    "    angles = [bin_0_45, bin_45_90, bin_0_45n, bin_45_90n]\n",
    "    lmax = 0\n",
    "    for j in range(len(angles)):\n",
    "        l = len(angles[j])\n",
    "        if l > lmax:\n",
    "            lmax = l\n",
    "            maxi = j\n",
    "    if lmax:\n",
    "        ans_arr = get_max_freq_elem(angles[maxi])\n",
    "        ans_res = np.mean(ans_arr)\n",
    "\n",
    "    else:\n",
    "        ans_arr = get_max_freq_elem(ap_deg)\n",
    "        ans_res = np.mean(ans_arr)\n",
    "    #### Rotate Image\n",
    "    angle = ans_res\n",
    "    if angle <= 30:\n",
    "        return img\n",
    "    if angle >= 0 and angle <= 90:\n",
    "        rot_angle = angle - 90\n",
    "    if angle >= -45 and angle < 0:\n",
    "        rot_angle = angle - 90\n",
    "    if angle >= -90 and angle < -45:\n",
    "        rot_angle = 90 + angle\n",
    "\n",
    "    rotated = rotate(img, rot_angle, resize=True, clip=True, mode='edge')\n",
    "    return rotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image(img,tol=.999):\n",
    "    # img is image data\n",
    "    # tol  is tolerance\n",
    "    mask = img>tol\n",
    "    return img[np.ix_(mask.any(1),mask.any(0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = crop_image(deskew(image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### This is used to get the rough coordinates below\n",
    "\n",
    "# shape = (int(np.mean([x.shape[1] for x in images])), int(np.mean([x.shape[0] for x in images]))) # Calculate mean shape\n",
    "# arr = np.zeros((shape[1], shape[0]))\n",
    "# count = 0\n",
    "# sample_count = 0 # Only take so many from each acc.\n",
    "# sampling = 10\n",
    "# errors = {}\n",
    "# for acc in tqdm(accessions):\n",
    "#     acc_path = root+acc\n",
    "#     acc_images = filter(lambda x: x != \".DS_Store\", os.listdir(acc_path))\n",
    "#     for image in tqdm(acc_images, leave=False):\n",
    "#         img_path = acc_path+'/'+image\n",
    "#         try:\n",
    "#             img = load_image(img_path)\n",
    "#             img = binarize(img)\n",
    "#             img = remove_noise(img)\n",
    "#             img = deskew(img)\n",
    "#             arr += cv.resize(img, shape)\n",
    "#             count += 1\n",
    "#         except Exception as e:\n",
    "#             errors[img_path] = e\n",
    "#         sample_count += 1\n",
    "    \n",
    "#         if sample_count >= sampling:\n",
    "#             sample_count = 0\n",
    "#             break\n",
    "#     imshow(arr/count, cmap='gray')\n",
    "#     plt.show()\n",
    "# np.savez('mean_image', arr/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load image\n",
    "# image = np.load('mean_image.npz')\n",
    "# image = image[image.keys()[0]]\n",
    "# # Create axis\n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(111)\n",
    "# ax.imshow(image, cmap='gray')\n",
    "\n",
    "# coords = []\n",
    "\n",
    "# def onclick(event):\n",
    "#     global ix, iy\n",
    "#     ix, iy = event.xdata, event.ydata\n",
    "#     print('(%d, %d),'%(ix, iy))\n",
    "#     global coords\n",
    "#     coords.append((ix, iy))\n",
    "#     return coords\n",
    "\n",
    "# cid = fig.canvas.mpl_connect('button_press_event', onclick)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_boxes(img):\n",
    "    img = Image.fromarray(img)\n",
    "    txt = pytesseract.image_to_data(img, config='-c tessedit_char_whitelist=abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789-()./\\\\\\'')\n",
    "    df = pd.read_csv(StringIO(txt), sep='\\t')\n",
    "    df = df.dropna()\n",
    "    return df\n",
    "df = to_boxes(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DF Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Upper Left, Lower left, Lower Right, Upper Right) Gotten from mean_image script\n",
    "info_rough_locations = {'CatNo': mplPath.Path([(35, 32), (43, 86), (394, 83), (392, 29)]),\n",
    "                        'AccNo': mplPath.Path([(26, 182), (32, 246), (386, 246), (378, 148)]),\n",
    "                        'OrigNo': mplPath.Path([(34, 327), (37, 388), (396, 394), (396, 317)]),\n",
    "                        'PhotoNo': mplPath.Path([(30, 467), (32, 536), (407, 528), (407, 458)]),\n",
    "                        'Situation': mplPath.Path([(42, 642), (45, 801), (1781, 818), (1774, 642)]),\n",
    "                        'Remarks': mplPath.Path([(21, 793), (39, 926), (1752, 969), (1741, 775)]),\n",
    "                        'Figured': mplPath.Path([(24, 936), (35, 1033), (1731, 1062), (1724, 972)]),\n",
    "                        'Name': mplPath.Path([(466, 6), (470, 103), (1727, 114), (1716, 6)]),\n",
    "                        'Site': mplPath.Path([(462, 153), (473, 254), (1576, 257), (1540, 121)]),\n",
    "                        'SiteNo': mplPath.Path([(470, 301), (477, 401), (1188, 408), (1152, 272)]),\n",
    "                        'Locality': mplPath.Path([(462, 444), (477, 602), (1724, 617), (1727, 419)])}\n",
    "\n",
    "f_to_acc = {'CatNo': 'Cat. No.',\n",
    "            'AccNo': 'Acc. No.',\n",
    "            'OrigNo': 'Orig. No.',\n",
    "            'PhotoNo': 'Photo No.',\n",
    "            'SiteNo': 'Site No.'}\n",
    "conv = lambda x: x if x not in f_to_acc.keys() else f_to_acc[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class SectionProcess(object):\n",
    "    @staticmethod\n",
    "    def _fuzzy_crop(key, s, preceding=False):\n",
    "\n",
    "        n = len(key.split())\n",
    "        arr = s.split()\n",
    "        pairs = [' '.join([arr[i2+i] for i2 in range(0,n)]) for i in range(0, len(arr)-n+1)]\n",
    "        pairs = process.extract(key, pairs, scorer=fuzz.token_sort_ratio)\n",
    "        pairs = filter(lambda x: x[1] > 50, pairs)\n",
    "        if len(pairs) == 0: ## No matches\n",
    "            if preceding:\n",
    "                return s\n",
    "            return s\n",
    "        pair = max(pairs, key=lambda x:x[1])[0]\n",
    "        try:\n",
    "            idx = s.index(pair)\n",
    "        except:\n",
    "            if preceding:\n",
    "                return s\n",
    "            return s\n",
    "        if preceding:\n",
    "            return s[:idx]\n",
    "        else:\n",
    "            return s[idx+len(pair):]\n",
    "        \n",
    "\n",
    "    @staticmethod\n",
    "    def fuzzy_crop(keys, s, preceding=False):\n",
    "        '''\n",
    "        Fuzzily crops the nearest string. \n",
    "        If preceding is true, it takes what is prior to that string,\n",
    "        otherwise what is after.\n",
    "        '''\n",
    "        if type(keys) is str:\n",
    "            keys = [keys]\n",
    "        for key in keys:\n",
    "            s = SectionProcess._fuzzy_crop(key, s, preceding)\n",
    "        return s\n",
    "    \n",
    "    @staticmethod\n",
    "    def grab_next(s):\n",
    "        a = s.split()\n",
    "        if len(a) == 0:\n",
    "            return s\n",
    "        return a[0]\n",
    "    \n",
    "    @staticmethod\n",
    "    def CatNo(s):\n",
    "        s = SectionProcess.fuzzy_crop('Name', s, preceding=True)\n",
    "        return SectionProcess.grab_next(s)\n",
    "    \n",
    "    @staticmethod\n",
    "    def AccNo(s):\n",
    "        return SectionProcess.grab_next(s)\n",
    "    \n",
    "#     @staticmethod\n",
    "#     def Name(s):\n",
    "#         return SectionProcess.fuzzy_crop(['Acc. No.', 'Acc.No.', 'Site'], s, preceding=True)\n",
    "    \n",
    "#     @staticmethod\n",
    "#     def OrigNo(s):\n",
    "#         return SectionProcess.fuzzy_crop(['Site No.', 'SiteNo.', 'Photo.'], s, preceding=True)\n",
    "    \n",
    "#     @staticmethod\n",
    "#     def Site(s):\n",
    "#         return SectionProcess.fuzzy_crop(['Orig No.', 'Orig No.'], s, preceding=True)\n",
    "\n",
    "#     @staticmethod\n",
    "#     def SiteNo(s):\n",
    "#         s = SectionProcess.fuzzy_crop('Locality', s)\n",
    "#         return SectionProcess.fuzzy_crop(['Photo. No.', 'Photo.No.'], s, preceding=True)\n",
    "    \n",
    "#     @staticmethod\n",
    "#     def PhotoNo(s):\n",
    "#         return SectionProcess.fuzzy_crop('Locality', s, preceding=True)\n",
    "    \n",
    "#     @staticmethod\n",
    "#     def Situation(s):\n",
    "#         return SectionProcess.fuzzy_crop('Remarks', s, preceding=True)\n",
    "    \n",
    "#     @staticmethod\n",
    "#     def Figured(s):\n",
    "#         return SectionProcess.fuzzy_crop('Remarks', s, preceding=True)\n",
    "    \n",
    "#     @staticmethod\n",
    "#     def Remarks(s):\n",
    "#         return SectionProcess.fuzzy_crop('Figured', s, preceding=True)\n",
    "    \n",
    "#     @staticmethod\n",
    "#     def Locality(s):\n",
    "#         return SectionProcess.fuzzy_crop('Locality', s, preceding=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_boxes(_df):\n",
    "    ret_dict = {}\n",
    "    guess = None\n",
    "    # To-DO: Create array of spans!\n",
    "    for key in info_rough_locations.keys():\n",
    "        tmp = _df[_df.apply(lambda x: bool(info_rough_locations[key].contains_point((x['left'] ,x['top']))), axis=1)]\n",
    "        guess = ' '.join(tmp['text']).strip()\n",
    "        guess = SectionProcess.fuzzy_crop(conv(key), guess)\n",
    "        if hasattr(SectionProcess, key):\n",
    "            guess = getattr(SectionProcess, key)(guess)\n",
    "        guess = guess.strip()\n",
    "        ret_dict[key] = guess\n",
    "    return ret_dict\n",
    "d = process_boxes(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Master DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=info_rough_locations.keys()+['f_name'])\n",
    "row = 0\n",
    "def process_image(img_path):\n",
    "    global boxes\n",
    "    d = {}\n",
    "    try:\n",
    "        img = load_image(img_path)\n",
    "        imshow(img, cmap='gray')\n",
    "        img = binarize(img)\n",
    "        img = remove_noise(img)\n",
    "        img = deskew(img)\n",
    "        boxes = to_boxes(img)\n",
    "        d = process_boxes(boxes)\n",
    "        d['AccNo'] = img_path.split('/')[-2]\n",
    "        d['f_name'] = img_path.split('/')[-1]\n",
    "        d['CatNo'] = img_path.split('/')[-1].split('_')[1].split('.')[0]\n",
    "    except Exception as e:\n",
    "        print(\"ERROR!\")\n",
    "        print(e)\n",
    "        print(d)\n",
    "    return d\n",
    "\n",
    "n = 8085\n",
    "def append_to_df(d):\n",
    "    global row, df\n",
    "    for key in d.keys():\n",
    "        df.loc[row,key] = d[key]\n",
    "    row += 1\n",
    "    # Save every n\n",
    "    if row % n == 0:\n",
    "        df.to_csv('april15_transcription.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# map(lambda x: append_to_df(process_image(x)), tqdm(paths))\n",
    "# p = Pool(6)\n",
    "# r = p.map(process_image, tqdm(paths))\n",
    "# p.close()\n",
    "# for val in tqdm(p.imap_unordered(process_image, paths), total=len(paths)):\n",
    "#     append_to_df(val)\n",
    "# p.close()\n",
    "errors = {}\n",
    "for path in tqdm(paths):\n",
    "    if df['f_name'].str.contains(path.split('/')[-1]).any():\n",
    "        continue\n",
    "    try:\n",
    "        append_to_df(process_image(path))\n",
    "    except Exception as e:\n",
    "        errors[path] = e\n",
    "df.to_csv('april15_transcription.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('april15_transcription.csv', index_col=0).replace(np.NaN, '')\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct Site Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO-DO: Update in time\n",
    "# Below is the list of site numbers created\n",
    "# ids = Counter(list(df['SiteNo']))\n",
    "# df[df['SiteNo'] == 'SiteNo. 159.152/1']\n",
    "# pprint(sorted(ids, key=ids.get, reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_nos = ['', 'Me. 152/7', 'Me. 152/1', 'M39/7', 'He. 152/7', 'He. 152/1', 'M39/7', 'He.152/1', 'M12/43', 'M49/31', 'Me.153/7', 'H35', 'M50/1', 'M13/77', 'Me. 158/7', 'Me. 158/1', 'M. 59/63', 'M 50/9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Correcting from prelabeled data in test5.csv\n",
    "### May need to be disabled in future.\n",
    "df['CatNo'] = pd.to_numeric(df['CatNo'], errors='coerce').fillna(-1).astype(np.int64)\n",
    "tmp = pd.read_csv('test5.csv')\n",
    "tmp = tmp[['Cat Number', 'Site Number']]\n",
    "tmp = tmp.dropna()\n",
    "tmp['Cat Number'] = pd.to_numeric(tmp['Cat Number'], errors='coerce').fillna(-1).astype(np.int64)\n",
    "df = pd.merge(tmp, df, left_on='Cat Number', right_on='CatNo', how='right')\n",
    "del df['Cat Number']\n",
    "df['Site Number'] = df['Site Number'].fillna(df['SiteNo'])\n",
    "df['SiteNo'] = df['Site Number']\n",
    "del df['Site Number']\n",
    "# df.head()\n",
    "### Nearest group\n",
    "assigned_groups = [min(site_nos, key=lambda g: editdistance.eval(g, k)) for k in df['SiteNo']]\n",
    "df['SiteNo'] = [y for x, y in zip(df['SiteNo'], assigned_groups)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spell Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace(np.NaN, '')\n",
    "corpi = ['google-10000-english-no-swears.txt',\n",
    "         'peabody_wordlists/site-list.csv',\n",
    "         'peabody_wordlists/name-list.csv',\n",
    "         'peabody_wordlists/peabody-lexicon.csv']\n",
    "### Fuzzy resegment\n",
    "def viterbi_segment(text):\n",
    "    probs, lasts = [1.0], [0]\n",
    "    for i in range(1, len(text) + 1):\n",
    "        prob_k, k = max((probs[j] * word_prob(text[j:i]), j)\n",
    "                        for j in range(max(0, i - max_word_length), i))\n",
    "        probs.append(prob_k)\n",
    "        lasts.append(k)\n",
    "    words = []\n",
    "    i = len(text)\n",
    "    while 0 < i:\n",
    "        words.append(text[lasts[i]:i])\n",
    "        i = lasts[i]\n",
    "    words.reverse()\n",
    "    return words, probs[-1]\n",
    "\n",
    "def word_prob(word): return dictionary[word] / total\n",
    "def words(text): return re.findall('[a-z]+', text.lower()) \n",
    "dictionary = Counter(words('\\n'.join([open(x).read() for x in corpi])))\n",
    "max_word_length = max(map(len, dictionary))\n",
    "total = float(sum(dictionary.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "post_punctuation = ['.', ',', ';', ':', '!', '/', '(', ')', '\\'', '\"']\n",
    "pre_punctuation = [\"'\"]\n",
    "correct_columns = ['Figured', 'Name', 'Locality', 'Situation', 'SiteNo', 'Site', 'PhotoNo', 'Remarks']\n",
    "site_list = pd.read_csv('corpi/site-list.csv', index_col=0)['Site Name']\n",
    "\n",
    "# def replace_common_mispellings(s):\n",
    "#     for key in common_mispellings.keys():\n",
    "#         s = s.replace(key, common_mispellings[key])\n",
    "#     return s\n",
    "\n",
    "def remove_punctuation_whitespace(s):\n",
    "    '''\n",
    "    Removes whitespace preceeding a punctuation\n",
    "    '''\n",
    "    s = re.sub(' +',' ', s) # Replace multiple spaces in row\n",
    "    for p in post_punctuation:\n",
    "        s = s.replace(' ' + p, p)\n",
    "    for p in pre_punctuation:\n",
    "        s = s.replace(p + ' ', p)\n",
    "    return s\n",
    "\n",
    "\n",
    "def textblob_it(s):\n",
    "    return str(TextBlob(s).correct())\n",
    "\n",
    "def site_correct(s):\n",
    "    tmp = s\n",
    "    suggestions = difflib.get_close_matches(s, site_list)\n",
    "    if len(suggestions) == 0:\n",
    "        return s\n",
    "    f = suggestions[0]\n",
    "    return f\n",
    "\n",
    "def resegment(s):\n",
    "    segmented = viterbi_segment(''.join(re.findall('[a-z]+', s.lower())))\n",
    "    if segmented[1] > 0:\n",
    "        ret_l = []\n",
    "        i = 0\n",
    "        l = []\n",
    "        while i < len(segmented[0]):\n",
    "            word = segmented[0][i]\n",
    "            l.append(word)\n",
    "            if len(word) > 1:\n",
    "                ret_l.append(''.join(l))\n",
    "                l = []\n",
    "            i += 1\n",
    "        ret_l.append(''.join(l))\n",
    "        return ' '.join(ret_l)\n",
    "            \n",
    "    return s\n",
    "\n",
    "normal_corrections = [remove_punctuation_whitespace, textblob_it]\n",
    "\n",
    "column_corrections = {\n",
    "    'Figured': normal_corrections,\n",
    "    'Name': normal_corrections,\n",
    "    'Locality': [resegment]+normal_corrections,\n",
    "    'Situation': normal_corrections,\n",
    "    'Site': [site_correct],\n",
    "    'Remarks': normal_corrections,\n",
    "}\n",
    "\n",
    "df = df.sample(frac=1)\n",
    "for row in tqdm(range(len(df))):\n",
    "    for col in column_corrections.keys():\n",
    "        s = df.loc[row, col]\n",
    "        if type(s) is not str:\n",
    "            continue\n",
    "        for aug in column_corrections[col]:\n",
    "            s = aug(s)\n",
    "        df.loc[row, col] = s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('april15_sc.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Correctness_Guess'] = np.zeros(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(row):\n",
    "    '''\n",
    "    Scores based on comprising words distance from actual words\n",
    "    '''\n",
    "    a = str(row['Name']).split() + str(row['Locality']).split() + str(row['Situation']).split()\n",
    "    if len(a) == 0:\n",
    "        return 1.0 # perfect grammar on a sentence that isn't there ;)\n",
    "    sc = 0\n",
    "    for word in a:\n",
    "        sc += Word(word).spellcheck()[0][1]\n",
    "    sc /= len(a)\n",
    "    return sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['Correctness_Guess'] = apply_by_multiprocessing(df, score, axis=1, workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('april15_sc_scored.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.sort_values('Correctness_Guess', ascending=False).replace(np.nan, '').head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
